{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain's components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Use the invoke method instead of direct calling\n",
    "print(llm.invoke('tell me a joke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"Translate the following English sentence to {language}:\n",
    "Sentence: {sentence}\n",
    "Translation:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Format the prompt and get the translation\n",
    "formatted_prompt = prompt.format(sentence=\"the cat is on the table\", language=\"spanish\")\n",
    "translation = llm.invoke(formatted_prompt)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    ['Name', 'Age', 'City'],\n",
    "    ['John', 25, 'New York'],\n",
    "    ['Emily', 28, 'Los Angeles'],\n",
    "    ['Michael', 22, 'Chicago']\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'sample.csv'\n",
    "\n",
    "# Write data to CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerows(data)\n",
    "\n",
    "print(f'Sample CSV file \"{file_name}\" generated and saved.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='sample.csv')\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences about mountains and nature\n",
    "content = \"\"\"Amidst the serene landscape, towering mountains stand as majestic guardians of nature's beauty.\n",
    "The crisp mountain air carries whispers of tranquility, while the rustling leaves compose a symphony of wilderness.\n",
    "Nature's palette paints the mountains with hues of green and brown, creating an awe-inspiring sight to behold.\n",
    "As the sun rises, it casts a golden glow on the mountain peaks, illuminating a world untouched and wild.\"\"\"\n",
    "\n",
    "# File name\n",
    "file_name = 'mountain.txt'\n",
    "\n",
    "# Write content to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    txtfile.write(content)\n",
    "\n",
    "# Read the file\n",
    "with open('mountain.txt') as f:\n",
    "    mountain = f.read()\n",
    "\n",
    "# Import from langchain.text_splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "texts = text_splitter.create_documents([mountain])\n",
    "\n",
    "# Print each text chunk\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "# Embed documents\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Good morning!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"I want to report an accident\",\n",
    "        \"Sorry to hear that. May I ask your name?\",\n",
    "        \"Sure, Mario Rossi.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print document embeddings information\n",
    "print(\"Embed documents:\")\n",
    "print(f\"Number of vector: {len(embeddings)}; Dimension of each vector: {len(embeddings[0])}\")\n",
    "\n",
    "# Embed query\n",
    "embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\")\n",
    "\n",
    "# Print query embedding information\n",
    "print(\"Embed query:\")\n",
    "print(f\"Dimension of the vector: {len(embedded_query)}\")\n",
    "print(f\"Sample of the first 5 elements of the vector: {embedded_query[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the conversation in a txt file\n",
    "# List of dialogue lines\n",
    "dialogue_lines = [\n",
    "    \"Good morning!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"I want to report an accident\",\n",
    "    \"Sorry to hear that. May I ask your name?\",\n",
    "    \"Sure, Mario Rossi.\"\n",
    "]\n",
    "\n",
    "# File name\n",
    "file_name = 'dialogue.txt'\n",
    "\n",
    "# Write dialogue lines to text file\n",
    "with open(file_name, 'w') as txtfile:\n",
    "    for line in dialogue_lines:\n",
    "        txtfile.write(line + '\\n')\n",
    "\n",
    "print(f'Dialogue text file \"{file_name}\" generated and saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "\n",
    "raw_documents = TextLoader('dialogue.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=0, separator = \"\\n\",)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the reason for calling?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Assuming 'db' is already defined from a previous vector store creation\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA chain with updated imports and initialization\n",
    "qa_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | PromptTemplate.from_template(\n",
    "        \"Given the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "    )\n",
    "    | OpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run the query\n",
    "query = \"What was the reason of the call?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create memory with the OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Save context to the memory\n",
    "memory.save_context(\n",
    "    {\"input\": \"hi, I'm looking for some ideas to write an essay in AI\"}, \n",
    "    {\"output\": \"hello, what about writing on LLMs?\"}\n",
    ")\n",
    "\n",
    "# Load memory variables\n",
    "memory_vars = memory.load_memory_variables({})\n",
    "print(memory_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConversationSummaryMemory.save_context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create memory with the OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# Save context to the memory\n",
    "memory.save_context(\n",
    "    {\"input\": \"hi, I'm looking for some ideas to write an essay in AI\"}, \n",
    "    {\"output\": \"hello, what about writing on LLMs?\"}\n",
    ")\n",
    "\n",
    "# Load memory variables\n",
    "memory_vars = memory.load_memory_variables({})\n",
    "print(memory_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "llm = OpenAI()\n",
    "\n",
    "# Define templates\n",
    "itinerary_template = \"\"\"You are a vacation itinerary assistant. \\\n",
    "You help customers finding the best destinations and itinerary. \\\n",
    "You help customer screating an optimized itinerary based on their preferences.\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"You are a restaurant booking assitant. \\\n",
    "You check with customers number of guests and food preferences. \\\n",
    "You pay attention whether there are special conditions to take into account.\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "# Prompt information\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"itinerary\",\n",
    "        \"description\": \"Good for creating itinerary\",\n",
    "        \"prompt_template\": itinerary_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"restaurant\",\n",
    "        \"description\": \"Good for help customers booking at restaurant\",\n",
    "        \"prompt_template\": restaurant_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create destination chains\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# Create default chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "# Prepare router destinations\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "\n",
    "# Create router prompt and chain\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "# Create multi-prompt chain\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"I'm planning a trip from Milan to Venice by car. What can I visit in between?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"I want to book a table for tonight\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the OpenAI language model\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Joke generation chain\n",
    "template = \"\"\"You are a comedian. Generate a joke on the following {topic}\n",
    "Joke:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"topic\"], template=template)\n",
    "joke_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Translation chain\n",
    "template = \"\"\"You are translator. Given a text input, translate it to {language}\n",
    "Translation:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "translator_chain = LLMChain(llm=llm, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the overall chain where we run these two chains in sequence.\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[joke_chain, translator_chain], verbose=True)\n",
    "translated_joke = overall_chain.run(\"Cats and Dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the string module\n",
    "import string\n",
    "\n",
    "# Define the function\n",
    "def rename_cat(inputs: dict) -> dict:\n",
    "  # Open the file in read mode\n",
    "  text = inputs[\"text\"]\n",
    "  # Create a table that maps punctuation characters to None\n",
    "  new_text = text.replace('cat', 'Silvester the Cat')\n",
    "  # Apply the table to the text and return the result\n",
    "  return {\"output_text\": new_text}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Read the file\n",
    "with open(\"Cats&Dogs.txt\") as f:\n",
    "    cats_and_dogs = f.read()\n",
    "\n",
    "import string\n",
    "\n",
    "# Define the rename_cat function (which was missing in the original code)\n",
    "def rename_cat(inputs):\n",
    "    text = inputs['text']\n",
    "    # Example transformation - replace 'cat' with 'feline'\n",
    "    return {'output_text': text.replace('cat', 'feline')}\n",
    "\n",
    "# Create transform chain\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], \n",
    "    output_variables=[\"output_text\"], \n",
    "    transform=rename_cat\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Summarize this text:\n",
    "{output_text}\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"output_text\"], template=template)\n",
    "\n",
    "# Create LLM chain\n",
    "llm_chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "\n",
    "# Create sequential chain\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, llm_chain])\n",
    "\n",
    "# Run the chain\n",
    "result = sequential_chain.run(cats_and_dogs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.serpapi import SerpAPIWrapper\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain_core.tools import Tool  # Updated import for Tool\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Make sure OpenAI API key is set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "# Initialize search and OpenAI\n",
    "search = SerpAPIWrapper()\n",
    "llm = OpenAI(temperature=0)  # Added temperature parameter for more factual responses\n",
    "\n",
    "# Define tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True  # Added error handling\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Run the agent\n",
    "    result = agent.invoke({\"input\": \"When was Avatar 2 released?\"})  # Updated to invoke() method\n",
    "    print(result[\"output\"])\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start working with LLMs in Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv   #installing the required package\n",
    "#!pip install huggingface_hub\n",
    "\n",
    "#option 1: get your tokens from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#option 2: get the token with the getpass function\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()\n",
    "HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify HF API token is set\n",
    "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "    raise ValueError(\"HUGGINGFACEHUB_API_TOKEN environment variable is not set\")\n",
    "\n",
    "# Define the question and template\n",
    "question = \"What was the first Disney movie?\"\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: give a direct answer\"\"\"\n",
    "\n",
    "# Create prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "class HuggingFaceHubWithTask(HuggingFaceHub):\n",
    "    def _call(self, prompt: str, **kwargs) -> str:\n",
    "        client = InferenceClient(\n",
    "            model=self.repo_id, \n",
    "            token=self.client.token\n",
    "        )\n",
    "        response = client.text_generation(\n",
    "            prompt,\n",
    "            temperature=self.model_kwargs.get(\"temperature\", 0.5),\n",
    "            max_new_tokens=self.model_kwargs.get(\"max_new_tokens\", 512)\n",
    "        )\n",
    "        # The response is already a string, no need to index it\n",
    "        return response\n",
    "\n",
    "# Initialize with Falcon model\n",
    "llm = HuggingFaceHubWithTask(\n",
    "    repo_id=\"tiiuae/falcon-7b-instruct\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_new_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Create and run the chain using the pipe syntax\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
